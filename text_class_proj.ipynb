{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f5c760-48d4-49e2-ae4b-f9777c19a55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from watclaimdata import test_df,train_df,valid_df\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras.layers import Embedding, Input, Dense, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_text as tf_text\n",
    "\n",
    "import sklearn as sk\n",
    "import os\n",
    "import nltk\n",
    "from nltk.data import find\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.test.utils import datapath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6fa8e3-7bf5-45c4-982f-2746f2425f64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa766c84-80bf-4413-8a22-fab750501d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('word2vec_sample')\n",
    "word2vec_sample = str(find('models/word2vec_sample/pruned.word2vec.txt'))\n",
    "model = KeyedVectors.load_word2vec_format(datapath(word2vec_sample), binary=False)\n",
    "\n",
    "EMBEDDING_DIM = len(model['university'])      \n",
    "\n",
    "# initialize embedding matrix and word-to-id map:  \n",
    "embedding_matrix = np.zeros((len(model) + 1, EMBEDDING_DIM))  \n",
    "vocab_dict = {}\n",
    "\n",
    "# build the embedding matrix and the word-to-id map:\n",
    "for i, word in enumerate(model.index_to_key):\n",
    "    embedding_vector = model[word]\n",
    "\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        vocab_dict[word] = i\n",
    "\n",
    "# we can use the last index at the end of the vocab for unknown tokens\n",
    "vocab_dict['[UNK]'] = len(vocab_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10b05d8-69a9-407e-9b90-2d13d9d65618",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['claim'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd5d1e5-24f0-43b8-bcfd-56406f6c92d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_examples = train_df['claim'].values.tolist()\n",
    "train_examples = tf.convert_to_tensor(train_examples)\n",
    "# train_labels = train_df['rating'].values.tolist()\n",
    "test_examples = test_df['claim'].values.tolist()\n",
    "test_examples = tf.convert_to_tensor(test_examples)\n",
    "# test_labels = test_df['rating'].values.tolist()\n",
    "val_examples = valid_df['claim'].values.tolist()\n",
    "val_examples = tf.convert_to_tensor(val_examples)\n",
    "val_labels = valid_df['rating'].values.tolist()\n",
    "\n",
    "# print('Train data contains {} with labels of {}'.format(train_examples.shape[0],train_labels['rating'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697531d6-4873-4956-b4dd-bf45e473d7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_examples[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3b62710-8ab2-4438-8ff8-3bb504c18d35",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_df\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrating\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mtolist()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_df' is not defined"
     ]
    }
   ],
   "source": [
    "train_df['rating'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8c5472eb-9f8c-4b5d-bc9c-5313a198a323",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function ndarray.tolist>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels = train_df['rating'].values.tolist\n",
    "# train_labels_array = np.array(train_labels)\n",
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "162a8886-64b7-49c6-ac81-6cb2a20d02fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_labels = pd.get_dummies(train_df['rating'])\n",
    "test_labels = pd.get_dummies(test_df['rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a500938a-87b1-4c74-8f7f-6aaf51eef208",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5,), dtype=string, numpy=\n",
       "array([b\"OpIndia claimed Greta Thunberg's real name is Ghazala bhat\",\n",
       "       b'\\xe2\\x80\\x9c38,000 prisoners were released from federal prison\\xe2\\x80\\x9d during the Obama administration.',\n",
       "       b'Says\\xc2\\xa0Wisconsin \\xe2\\x80\\x98could compel\\xe2\\x80\\x99 Foxconn to install solar panels that would power 33,000 homes.',\n",
       "       b\"A 4-year-old boy was accused of hacking the FBI's databases.\",\n",
       "       b'Whistleblower accusing Philippine VP of fraud surfaces in December 2020'],\n",
       "      dtype=object)>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_examples[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "aa0da942-b42e-4eeb-8b45-edba6a11bb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.convert_to_tensor(train_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "999aea3d-0083-448a-8636-b9adb4933d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tf_text.WhitespaceTokenizer()\n",
    "train_tokens = tokenizer.tokenize(train_examples)\n",
    "test_tokens = tokenizer.tokenize(test_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5326cf02-0d0c-4c04-9e57-c5092a8d0f35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(9,), dtype=string, numpy=\n",
       "array([b'OpIndia', b'claimed', b'Greta', b\"Thunberg's\", b'real', b'name',\n",
       "       b'is', b'Ghazala', b'bhat'], dtype=object)>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bc8bb52a-baa4-405b-b782-224bc32e6fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 128\n",
    "\n",
    "def docs_to_vocab_ids(tokenized_texts_list):\n",
    "    \"\"\"\n",
    "    converting a list of strings to a list of lists of word ids\n",
    "    \"\"\"\n",
    "    texts_vocab_ids = []\n",
    "    for i, token_list in enumerate(tokenized_texts_list):\n",
    "\n",
    "        # Get the vocab id for each token in this doc ([UNK] if not in vocab)\n",
    "        vocab_ids = []\n",
    "        for token in list(token_list.numpy()):\n",
    "            decoded = token.decode('utf-8', errors='ignore')\n",
    "            if decoded in vocab_dict:\n",
    "                vocab_ids.append(vocab_dict[decoded])\n",
    "            else:\n",
    "                vocab_ids.append(vocab_dict['[UNK]'])\n",
    "            \n",
    "        # Truncate text to max length, add padding up to max length\n",
    "        vocab_ids = vocab_ids[:MAX_SEQUENCE_LENGTH]\n",
    "        n_padding = (MAX_SEQUENCE_LENGTH - len(vocab_ids))\n",
    "        # For simplicity in this model, we'll just pad with uknown tokens\n",
    "        vocab_ids += [vocab_dict['[UNK]']] * n_padding\n",
    "\n",
    "        # Add this example to the list of converted docs\n",
    "        texts_vocab_ids.append(vocab_ids)\n",
    "            \n",
    "        if i % 5000 == 0:\n",
    "            print('Examples processed: ', i)\n",
    "        \n",
    "    print('Total examples: ', i)\n",
    "\n",
    "    return np.array(texts_vocab_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "374bd4ef-f566-4f1b-9e8c-8776dc8cf1f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples processed:  0\n",
      "Examples processed:  5000\n",
      "Examples processed:  10000\n",
      "Examples processed:  15000\n",
      "Examples processed:  20000\n",
      "Examples processed:  25000\n",
      "Total examples:  26975\n",
      "Examples processed:  0\n",
      "Total examples:  3372\n"
     ]
    }
   ],
   "source": [
    "train_input = docs_to_vocab_ids(train_tokens)\n",
    "test_input = docs_to_vocab_ids(test_tokens)\n",
    "\n",
    "train_labels = np.array(train_labels)\n",
    "test_labels = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "41cf39dd-3ed6-4f37-ade3-6dfe83757b74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bfc6c5b8-2231-4046-8536-82321c9f698c",
   "metadata": {},
   "outputs": [],
   "source": [
    "an_input_layer = tf.keras.layers.Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int64')\n",
    "\n",
    "an_embedding_layer = Embedding(embedding_matrix.shape[0],\n",
    "                               embedding_matrix.shape[1],\n",
    "                               embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n",
    "                               input_length=MAX_SEQUENCE_LENGTH,\n",
    "                               trainable=False)\n",
    "\n",
    "an_embeddings = an_embedding_layer(an_input_layer)\n",
    "\n",
    "an_avg_embeddings = tf.keras.layers.Lambda(lambda x: K.mean(x, axis=1), name='averaging')(an_embeddings)\n",
    "\n",
    "an_classification = tf.keras.layers.Dense(3, \n",
    "                                          activation='softmax', \n",
    "                                          name='an_classification')(an_avg_embeddings)\n",
    "\n",
    "an_model = tf.keras.models.Model(inputs=an_input_layer, outputs=[an_classification])\n",
    "\n",
    "\n",
    "an_model.compile(loss='binary_crossentropy',\n",
    "                 optimizer=tf.keras.optimizers.Adam(learning_rate=0.001,\n",
    "                                                    beta_1=0.9,\n",
    "                                                    beta_2=0.999,\n",
    "                                                    epsilon=1e-07,\n",
    "                                                    amsgrad=False,\n",
    "                                                    name='Adam'),\n",
    "                 metrics='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a9e3d78b-0e70-437c-8d2d-738a6f90ab5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 128)]             0         \n",
      "                                                                 \n",
      " embedding_1 (Embedding)     (None, 128, 300)          13194600  \n",
      "                                                                 \n",
      " averaging (Lambda)          (None, 300)               0         \n",
      "                                                                 \n",
      " an_classification (Dense)   (None, 3)                 903       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 13195503 (50.34 MB)\n",
      "Trainable params: 903 (3.53 KB)\n",
      "Non-trainable params: 13194600 (50.33 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "an_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "77292fdc-8afb-4dc4-8756-f61e1e516679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m an_history \u001b[38;5;241m=\u001b[39m \u001b[43man_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtest_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_labels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                          \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:857\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    854\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    855\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    856\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 857\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    859\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    860\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not callable"
     ]
    }
   ],
   "source": [
    "an_history = an_model.fit(train_input,\n",
    "                          train_labels,\n",
    "                          validation_data=(test_input, test_labels),\n",
    "                          batch_size=32,\n",
    "                          epochs=20\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c153b280-2c39-49bc-a695-73f7c226c075",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26976, 3)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db342d0a-dc41-494f-8b61-0f57bdf2bfc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
